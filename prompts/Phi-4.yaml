_id: Phi-4
templateFormat: hf
type: system
version:
  reasoning:
    supports:
      - thinkMode: ['deep']
    shouldThink:
      mode: deep
      thinkTag: ["<think>", "</think>"]
prompt:
  system: You are a medieval knight and must provide explanations to modern people.
  bot_token: '<|im_start|>'
  sep_token: "<|im_sep|>"
  eot_token: "<|im_end|>"
template: |-
  {%- for message in messages %}
    {% if loop.first and messages[0]['role'] != 'system' %}
      {{- bot_token + 'system' + sep_token + '\n' + system + eot_token + '\n'-}}
    {% endif %}
    {{- bot_token + message['role'] + sep_token + '\n' + message['content'] -}}
    {%- if not loop.last or messages[-1]['role'] != 'assistant' %}{{ eot_token + '\n' }}{% endif %}
  {%- endfor -%}
  {%- if add_generation_prompt and messages[-1]['role'] != 'assistant' -%}
      {{- bot_token + 'assistant' + sep_token -}}
  {%- endif -%}
modelPattern:
  reasoning: !re /(?:^|[-_.])(phi)[-_]4[-_]reasoning(?:$|[-_.])/i
  "@": !re /(?:^|[-_.])(phi)[-_]4(?:$|[-_.])/i
parameters:
  'reasoning':
    temperature: 0.8
    top_p: 0.95
    top_k: 50
    seed: -1 # setting it to -1 implies that a different response will be generated on each generation, similarly to HuggingFace's `do_sample` arg. Defaults to 1337.